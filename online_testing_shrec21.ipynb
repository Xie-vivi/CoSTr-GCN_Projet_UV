{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e20f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77f5ff02",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95896aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import online_test_loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b7768ae",
   "metadata": {},
   "source": [
    "### Define params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f936900",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "# model params\n",
    "window_size = 50\n",
    "memory_size = 50\n",
    "stride = 5\n",
    "input_shape = (memory_size, 20, 3)\n",
    "num_classes = 18\n",
    "\n",
    "#  change this if you train a new model with other hyperparams\n",
    "num_heads = 8\n",
    "d_model = 128\n",
    "n_heads = 8\n",
    "dropout_rate = .3\n",
    "dataset_name = \"SHREC21\"\n",
    "\n",
    "model_path = f\"models\\\\CoSTrGCN-model\\\\{dataset_name}\\\\best_model-128-8-v1.ckpt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec08cd1d",
   "metadata": {},
   "source": [
    "### Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77a3491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHREC21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:23<00:00,  4.65it/s]\n",
      "100%|██████████| 72/72 [00:14<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHREC21\n",
      "test data num:  72\n",
      "\n",
      " loading model.............\n",
      "********** Testing **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [17:50, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m online_test_loop(model_path,\n\u001b[0;32m      2\u001b[0m                     window_size,\n\u001b[0;32m      3\u001b[0m                     dataset_name,\n\u001b[0;32m      4\u001b[0m                     num_classes,\n\u001b[0;32m      5\u001b[0m                     stride,\n\u001b[0;32m      6\u001b[0m                     memory_size,\n\u001b[0;32m      7\u001b[0m                     dropout_rate,\n\u001b[0;32m      8\u001b[0m                     d_model,\n\u001b[0;32m      9\u001b[0m                     num_heads\n\u001b[0;32m     10\u001b[0m                     )\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\utils\\online_evaluation_utils.py:222\u001b[0m, in \u001b[0;36monline_test_loop\u001b[1;34m(model_path, window_size, dataset_name, num_classes, stride, memory_size, dropout_rate, d_model, num_heads)\u001b[0m\n\u001b[0;32m    219\u001b[0m label \u001b[39m=\u001b[39m get_window_label(label_l, num_classes)\n\u001b[0;32m    221\u001b[0m tic()\n\u001b[1;32m--> 222\u001b[0m score \u001b[39m=\u001b[39m model(window)\n\u001b[0;32m    224\u001b[0m prob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(score, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    226\u001b[0m score_list_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(prob, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Wael\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\model.py:72\u001b[0m, in \u001b[0;36mCoSTrGCN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn(x,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjacency_matrix)\n\u001b[0;32m     71\u001b[0m \u001b[39m# temporal features from TGE\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     74\u001b[0m \u001b[39m# Global average pooling\u001b[39;00m\n\u001b[0;32m     75\u001b[0m N,T,V,C\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\Wael\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:537\u001b[0m, in \u001b[0;36mTransformerGraphEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    535\u001b[0m x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoder(x)\n\u001b[0;32m    536\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 537\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[0;32m    538\u001b[0m \u001b[39m# if self.call_mode==CallMode.FORWARD_STEPS:\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39m#     self.clean_state()\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Wael\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:470\u001b[0m, in \u001b[0;36mTransformerGraphEncoderLayer.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 470\u001b[0m     src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(src))\n\u001b[0;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(src)\n",
      "File \u001b[1;32mc:\\Users\\Wael\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:56\u001b[0m, in \u001b[0;36mResidual.forward\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     54\u001b[0m     \u001b[39m# Assume that the \"query\" tensor is given first, so we can compute the\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[39m# residual.  This matches the signature of 'MultiHeadAttention'.\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msublayer(\u001b[39m*\u001b[39;49mtensors))\n\u001b[0;32m     57\u001b[0m     \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[39m# print(tensors[0].shape)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     x\u001b[39m=\u001b[39mtensors[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\Wael\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\continual\\module.py:286\u001b[0m, in \u001b[0;36mCoModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks\n\u001b[0;32m    280\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mor\u001b[39;00m _global_forward_pre_hooks\n\u001b[0;32m    285\u001b[0m ):\n\u001b[1;32m--> 286\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    287\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m    288\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:434\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward_steps\u001b[1;34m(self, x, pad_end, update_state)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_steps\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, pad_end\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, update_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    433\u001b[0m     out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(\n\u001b[1;32m--> 434\u001b[0m         torch\u001b[39m.\u001b[39mcat([h\u001b[39m.\u001b[39mforward_steps(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    435\u001b[0m     )\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:434\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_steps\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, pad_end\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, update_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    433\u001b[0m     out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(\n\u001b[1;32m--> 434\u001b[0m         torch\u001b[39m.\u001b[39mcat([h\u001b[39m.\u001b[39;49mforward_steps(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    435\u001b[0m     )\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:358\u001b[0m, in \u001b[0;36mAttentionHead.forward_steps\u001b[1;34m(self, x, update_state)\u001b[0m\n\u001b[0;32m    355\u001b[0m outs \u001b[39m=\u001b[39m []\n\u001b[0;32m    357\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[1;32m--> 358\u001b[0m     o, tmp_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_step(query[t], key[t], value[t], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_size, tmp_state)\n\u001b[0;32m    359\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, Tensor):\n\u001b[0;32m    360\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first:\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:252\u001b[0m, in \u001b[0;36mAttentionHead._forward_step\u001b[1;34m(self, query, key, value, T, prev_state)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m prev_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     prev_state \u001b[39m=\u001b[39m (\n\u001b[0;32m    248\u001b[0m         \u001b[39m*\u001b[39m_scaled_dot_product_attention_default_state(B, T, V, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_k, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_v),\n\u001b[0;32m    249\u001b[0m         \u001b[39m-\u001b[39mT,\n\u001b[0;32m    250\u001b[0m     )\n\u001b[1;32m--> 252\u001b[0m o, new_state \u001b[39m=\u001b[39m _scaled_dot_product_attention_step(\n\u001b[0;32m    253\u001b[0m     prev_state[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[0;32m    254\u001b[0m     query,\n\u001b[0;32m    255\u001b[0m     key,\n\u001b[0;32m    256\u001b[0m     value,\n\u001b[0;32m    257\u001b[0m     T,\n\u001b[0;32m    258\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout,\n\u001b[0;32m    259\u001b[0m )\n\u001b[0;32m    260\u001b[0m stride_index \u001b[39m=\u001b[39m prev_state[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m stride_index \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Wael\\Desktop\\folders\\CoSTrGCN\\layers\\continual_transformer_layers.py:132\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention_step\u001b[1;34m(prev_state, q_step, k_step, v_step, T, dropout_p)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39m# Update states\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m# Note: We're allowing the K and V mem to have one more entry than\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m# strictly necessary to simplify computatations.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m K_T_new \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mroll(K_T_mem, shifts\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dims\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m,))\n\u001b[1;32m--> 132\u001b[0m K_T_new[:B, :, :, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m=\u001b[39m k_step\n\u001b[0;32m    133\u001b[0m V_new \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mroll(V_mem, shifts\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dims\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m,))\n\u001b[0;32m    134\u001b[0m V_new[:B, :, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m v_step\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "online_test_loop(model_path,\n",
    "                    window_size,\n",
    "                    dataset_name,\n",
    "                    num_classes,\n",
    "                    stride,\n",
    "                    memory_size,\n",
    "                    dropout_rate,\n",
    "                    d_model,\n",
    "                    num_heads\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Mar 26 2020, 10:43:30) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e84d36fea9bb7168ec52ba4b2cced9e0716675f4aa44d2e66f414a9cb918fe0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
